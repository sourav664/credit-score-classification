{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2fa214ae-e172-42a0-ba25-b5ec344977c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install optuna xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f057e886-1807-4e53-9665-8b1f45eaccbb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas\n",
    "import optuna\n",
    "import mlflow\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import roc_auc_score, f1_score, accuracy_score, classification_report\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b11bf7a-bc25-4da8-bc09-daacd7e4bef7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c4f40f6-9caf-427c-88f0-22fd3ce4da2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_df = spark.table(\"credit_catalog.gold.ml_train\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8578b7d8-a220-4262-a4bc-018cbfa2c684",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "X = train_df.drop(columns=['Credit_Score'])\n",
    "y = train_df['Credit_Score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ad81047-7b84-446c-8923-52bb06aadc95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7c18d10-40b4-4b41-9ffb-0e4121e6b46a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# train test split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4fb27dca-dc31-4c77-aaf9-1b8c82c4f35f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"The size of train data is\",X_train.shape)\n",
    "print(\"The shape of test data is\",X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04d8df21-26eb-4531-91d5-1f68159ed2a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ohe_encode = ['Occupation','Income_Group', 'Payment_of_Min_Amount']\n",
    "ordinal_encode = ['Credit_Mix']\n",
    "standard_scale = ['Age', 'Monthly_Inhand_Salary', 'Num_Bank_Accounts', 'Num_Credit_Card',\n",
    "       'Interest_Rate', 'Num_of_Loan', 'Delay_from_due_date',\n",
    "       'Num_of_Delayed_Payment', 'Changed_Credit_Limit',\n",
    "       'Num_Credit_Inquiries', 'Outstanding_Debt', 'Credit_Utilization_Ratio',\n",
    "       'Credit_History_Age', 'Total_EMI_per_month', 'Amount_invested_monthly',\n",
    "       'Monthly_Balance', 'has_auto_loan', 'has_credit_builder_loan',\n",
    "       'has_debt_consolidation_loan', 'has_home_equity_loan',\n",
    "       'has_mortgage_loan', 'has_payday_loan', 'has_personal_loan',\n",
    "       'has_student_loan', 'spend_level', 'txn_value_level',\n",
    "       'Debt_to_Income_Ratio', 'EMI_to_Salary_Ratio', 'Saving_Capacity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12b05cff-790b-45eb-8730-5d9296b27f84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('ohe',\n",
    "         OneHotEncoder(drop='first',\n",
    "                       sparse_output=False,\n",
    "                       handle_unknown='ignore'),\n",
    "         ohe_encode),\n",
    "\n",
    "        ('ordinal',\n",
    "         OrdinalEncoder(categories=[['bad', 'good', 'standard']]),\n",
    "         ordinal_encode),\n",
    "\n",
    "        ('standard',\n",
    "         StandardScaler(),\n",
    "         standard_scale)\n",
    "    ],\n",
    "    remainder=\"passthrough\",\n",
    "    n_jobs=-1,\n",
    "    force_int_remainder_cols=False,\n",
    "    verbose_feature_names_out=False\n",
    ")\n",
    "\n",
    "preprocessor.set_output(transform=\"pandas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e0bfe77-223e-486a-9991-3b14c3b4de02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "X_train_trans = preprocessor.fit_transform(X_train)\n",
    "X_test_trans = preprocessor.transform(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6386d0bb-7520-493e-b277-b0ca99314f6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Title: Compute class weights and sample weights for imbalanced classification\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "classes = np.unique(y_train)\n",
    "\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=classes,\n",
    "    y=y_train\n",
    ")\n",
    "\n",
    "class_weight_dict = dict(zip(classes, class_weights))\n",
    "\n",
    "# Convert to sample weights\n",
    "sample_weight = np.array([class_weight_dict[label] for label in y_train])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2dd557f4-d89a-4622-b34f-031a0acdfff8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f713828e-3bb7-4d2f-ba49-79dc81a8f784",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Train and log model with correct input for mlflow"
    }
   },
   "outputs": [],
   "source": [
    "TARGET = \"Credit_Score\"\n",
    "\n",
    "# train the model on best parameters\n",
    "xgb_params = {'n_estimators': 160, \n",
    "              'learning_rate': 0.13517507315287902, \n",
    "              'max_depth': 10, \n",
    "              'min_child_weight': 3, \n",
    "              'gamma': 0.19187053861391912, \n",
    "              'subsample': 0.7453261407003142, \n",
    "              'colsample_bytree': 0.9022795822470242, \n",
    "              'reg_lambda': 52.61564564741222}\n",
    "    \n",
    "# ---------------------------\n",
    "# Train + log model\n",
    "\n",
    "# train the model on best parameters\n",
    "best_xgb = XGBClassifier(**xgb_params)\n",
    "\n",
    "full_pipe = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model',best_xgb),\n",
    "    ])\n",
    "\n",
    "# train the model\n",
    "full_pipe.fit(X_train,y_train, model__sample_weight=sample_weight )\n",
    "\n",
    "# get the predictions\n",
    "y_pred_train = full_pipe.predict(X_train)\n",
    "y_pred_test = full_pipe.predict(X_test)\n",
    "\n",
    "y_train_proba = full_pipe.predict_proba(X_train)\n",
    "y_test_proba = full_pipe.predict_proba(X_test)\n",
    "\n",
    "y_pred_train = y_train_proba.argmax(axis=1)\n",
    "y_pred_test = y_test_proba.argmax(axis=1)\n",
    "\n",
    "cv_scores = cross_val_score(full_pipe,\n",
    "                         X_train,\n",
    "                         y_train,\n",
    "                         scoring=\"roc_auc_ovr\",\n",
    "                         cv=5,n_jobs=-1)\n",
    "\n",
    "with mlflow.start_run(run_name=\"final_model_training\") as run:\n",
    "    # set tags\n",
    "    mlflow.set_tag(\"model\",\" Credit Score Classification\")\n",
    " \n",
    "    # log parameters\n",
    "    mlflow.log_params(xgb_params)\n",
    "\n",
    "    # CV metric (already correct)\n",
    "    mlflow.log_metric(\"cv_roc_auc_ovr\", cv_scores.mean())\n",
    "\n",
    "    #  ROC-AUC (USE PROBABILITIES)\n",
    "    mlflow.log_metric(\n",
    "        \"train_roc_auc_ovr\",\n",
    "        roc_auc_score(y_train, y_train_proba, multi_class=\"ovr\")\n",
    "    )\n",
    "\n",
    "    mlflow.log_metric(\n",
    "        \"test_roc_auc_ovr\",\n",
    "        roc_auc_score(y_test, y_test_proba, multi_class=\"ovr\")\n",
    "    )\n",
    "     # F1 (USE macro for multiclass)\n",
    "    mlflow.log_metric(\n",
    "        \"train_f1_macro\",\n",
    "        f1_score(y_train, y_pred_train, average=\"macro\")\n",
    "    )\n",
    "\n",
    "    mlflow.log_metric(\n",
    "        \"test_f1_macro\",\n",
    "        f1_score(y_test, y_pred_test, average=\"macro\")\n",
    "    )\n",
    "\n",
    "    #  Accuracy (USES LABELS)\n",
    "    mlflow.log_metric(\"train_accuracy\", accuracy_score(y_train, y_pred_train))\n",
    "    mlflow.log_metric(\"test_accuracy\", accuracy_score(y_test, y_pred_test))\n",
    "\n",
    "    # log individual cv scores\n",
    "    mlflow.log_metrics({f\"CV {num}\": score for num, score in enumerate(-cv_scores)})\n",
    "\n",
    "    # mlflow dataset input datatype\n",
    "    train_data_input = mlflow.data.from_pandas(train_df, targets=TARGET)\n",
    "    val_df = X_test.copy()\n",
    "    val_df[TARGET] = y_test\n",
    "    test_data_input = mlflow.data.from_pandas(val_df, targets=TARGET)\n",
    "    \n",
    "    # log input\n",
    "    mlflow.log_input(dataset=train_data_input,context=\"training\")\n",
    "    mlflow.log_input(dataset=test_data_input,context=\"validation\")\n",
    "\n",
    "    # model signature\n",
    "    model_signature = mlflow.models.infer_signature(model_input=X_train.sample(20,random_state=42),\n",
    "                                model_output=full_pipe.predict(X_train.sample(20,random_state=42)))\n",
    "    \n",
    "    # log the final model\n",
    "    mlflow.sklearn.log_model(full_pipe,\"credit_score_classification\",signature=model_signature)\n",
    "\n",
    "    # get the current run artifact uri\n",
    "    artifact_uri = mlflow.get_artifact_uri()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fc9b55a-3ba5-43dd-a3ec-f421c262a0e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE VOLUME IF NOT EXISTS credit_catalog.credit_schema.run_information;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82a96a77-35d4-49ec-97ec-0d4924e6f496",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "def save_model_info(root_path,save_json_path, run_id, artifact_path, model_name):\n",
    "    info_dict = {\n",
    "        \"run_id\": run_id,\n",
    "        \"artifact_path\": artifact_path,\n",
    "        \"model_name\": model_name\n",
    "    }\n",
    "    run_id_path = os.path.join(root_path, save_json_path)\n",
    "    with open(run_id_path, \"w\") as f:\n",
    "        json.dump(info_dict, f, indent=4)\n",
    "\n",
    "\n",
    "# ðŸ”¹ Use DBFS path\n",
    "root_path = Path(\n",
    "    \"/Volumes/credit_catalog/credit_schema/run_information\"\n",
    ")\n",
    "\n",
    "run_id = run.info.run_id\n",
    "model_name = \"credit_score_classification\"\n",
    "artifact_uri = f\"runs:/{run_id}/model\"\n",
    "\n",
    "save_json_path = \"run_information.json\"\n",
    "\n",
    "save_model_info(\n",
    "    root_path=root_path,\n",
    "    save_json_path=save_json_path,\n",
    "    run_id=run_id,\n",
    "    artifact_path=artifact_uri,\n",
    "    model_name=model_name\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea15d01a-3a2f-4100-8aec-76c4056dee0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6170991146224809,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "model_training",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
