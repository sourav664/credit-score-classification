{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5664d792-3419-4f12-903e-4467a9ec9290",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Read Bronze table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9694671a-c626-4c2f-bfac-9bb107ffd659",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_df = spark.table(\"credit_catalog.bronze.train\")\n",
    "test_df = spark.table(\"credit_catalog.bronze.test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b67e3c2-6b9a-48e2-98b3-ca0beac9395a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Silver Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bed2675-1618-4842-9d9c-97bcd56878c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Age\n",
    "- **Data Type**: Convert to integer\n",
    "- **Range**: 18-65 years (typical for financial datasets)\n",
    "- **Data Quality Issues:**\n",
    "  - Invalid values such as `-500`, `23_`, `8200` etc. are present (data errors)\n",
    "  - Some missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48ad413b-9440-4c67-aa58-6dc4b8660634",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Filtering Consistent Age Values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b373418-b914-4dba-bdf0-5f47f99a1e67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace, col, when\n",
    "\n",
    "def clean_age_column(df, age_col=\"Age\"):\n",
    "    \"\"\"\n",
    "    Cleans and validates the Age column.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    df = (\n",
    "        df\n",
    "        # Remove \"_\" placeholder\n",
    "        .withColumn(\n",
    "            age_col,\n",
    "            regexp_replace(col(age_col), \"_\", \"\")\n",
    "        )\n",
    "        # Convert empty string to NULL\n",
    "        .withColumn(\n",
    "            age_col,\n",
    "            when(col(age_col) == \"\", None).otherwise(col(age_col))\n",
    "        )\n",
    "        # Cast to integer\n",
    "        .withColumn(\n",
    "            age_col,\n",
    "            col(age_col).cast(\"int\")\n",
    "        )\n",
    "        # Filter valid ages\n",
    "        .filter(\n",
    "            col(age_col).isNotNull() &\n",
    "            (col(age_col) >= 18) &\n",
    "            (col(age_col) < 65)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "train_df = clean_age_column(train_df, age_col=\"Age\")\n",
    "test_df = clean_age_column(test_df, age_col=\"Age\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4042db56-d0ea-4563-91fe-054577672c49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, when, last, first\n",
    "\n",
    "# Convert Month name → Month number first (important)\n",
    "train_df = train_df.withColumn(\n",
    "    \"month_num\",\n",
    "    when(col(\"Month\") == \"January\", 1)\n",
    "    .when(col(\"Month\") == \"February\", 2)\n",
    "    .when(col(\"Month\") == \"March\", 3)\n",
    "    .when(col(\"Month\") == \"April\", 4)\n",
    "    .when(col(\"Month\") == \"May\", 5)\n",
    "    .when(col(\"Month\") == \"June\", 6)\n",
    "    .when(col(\"Month\") == \"July\", 7)\n",
    "    .when(col(\"Month\") == \"August\", 8)\n",
    "    \n",
    ")\n",
    "\n",
    "test_df = test_df.withColumn(\n",
    "    \"month_num\",\n",
    "    when(col(\"Month\") == \"September\", 9)\n",
    "    .when(col(\"Month\") == \"October\", 10)\n",
    "    .when(col(\"Month\") == \"November\", 11)\n",
    "    .when(col(\"Month\") == \"December\", 12)\n",
    "    \n",
    "    \n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35b04f90-e41c-4c24-82f7-57914f567a06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##  Occupation\n",
    "- **Data Type**: String (Categorical)\n",
    "- **Description**: Professional occupation or job title of the customer\n",
    "- **Example Values**: `Scientist`, `Teacher`, `Engineer`, `Entrepreneur`, `Developer`\n",
    "- **Purpose**: Occupational classification for risk profiling\n",
    "- **Data Quality Issues**:\n",
    "  - Some missing values represented as `_______`\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d911f50f-cbb5-4106-868b-af75cd1aa419",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Replacing Inconsistent Occupation Values\n",
    "- We are going to use the `Customer_ID` and `month` columns to replace the `Occupation` column. Since `Customer_ID` contains records for each month, we will leverage this information to replace inconsistent occupation values using `forward fill` and `backward fill` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9b78d33-c698-44a1-b573-15ae320b94d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, when, last, first, lower\n",
    "\n",
    "def clean_occupation_column(\n",
    "    df,\n",
    "    occupation_col=\"Occupation\",\n",
    "    partition_col=\"Customer_ID\",\n",
    "    order_col=\"Month_num\"\n",
    "    \n",
    "):\n",
    "    \"\"\"\n",
    "    Cleans and imputes the Occupation column using time-aware logic.\n",
    "    \"\"\"\n",
    "\n",
    "    w = Window.partitionBy(partition_col).orderBy(order_col)\n",
    "\n",
    "    df = (\n",
    "        df\n",
    "        # Step 1: mark invalid occupation as NULL\n",
    "        .withColumn(\n",
    "            occupation_col,\n",
    "            when(col(occupation_col) == \"_______\", None)\n",
    "            .otherwise(col(occupation_col))\n",
    "        )\n",
    "        # Step 2: forward fill\n",
    "        .withColumn(\n",
    "            occupation_col,\n",
    "            last(occupation_col, ignorenulls=True).over(w)\n",
    "        )\n",
    "        # Step 3: backward fill\n",
    "        .withColumn(\n",
    "            occupation_col,\n",
    "            first(occupation_col, ignorenulls=True)\n",
    "            .over(w.rowsBetween(0, Window.unboundedFollowing))\n",
    "        )\n",
    "        # Step 4: standardize to lowercase\n",
    "        .withColumn(\n",
    "            occupation_col,\n",
    "            lower(col(occupation_col))\n",
    "        )\n",
    "    )\n",
    "\n",
    "    \n",
    "\n",
    "    return df\n",
    "\n",
    "train_df = clean_occupation_column(train_df)\n",
    "test_df = clean_occupation_column(test_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12979c53-da09-4a3a-b89e-daa5f6cb54cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Annual_Income\n",
    "- **Data Type**: Convert to Float/Decimal (Numeric)\n",
    "- **Description**: Total annual income of the customer in currency units\n",
    "- **Example Values**: `19114.12`, `34847.84`, `143162.64`, `30689.89`, `35547.71`\n",
    "- **Unit**: Currency (e.g., USD)\n",
    "- **Purpose**: Income-based credit assessment and risk evaluation\n",
    "- **Data Quality Issues**:\n",
    "  - Some values may have trailing underscores like `34847.84_`\n",
    "  - Indicates inconsistent data entry\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b63a2e40-45d0-433c-92b2-2c741fa72b79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace, col, when, round\n",
    "\n",
    "def clean_annual_income_column(df, income_col=\"Annual_Income\"):\n",
    "    \"\"\"\n",
    "    Cleans and standardizes the Annual_Income column.\n",
    "    \"\"\"\n",
    "\n",
    "    df = (\n",
    "        df\n",
    "        # Step 1: remove \"_\" placeholder\n",
    "        .withColumn(\n",
    "            income_col,\n",
    "            regexp_replace(col(income_col), \"_\", \"\")\n",
    "        )\n",
    "        # Step 2: convert empty string to NULL\n",
    "        .withColumn(\n",
    "            income_col,\n",
    "            when(col(income_col) == \"\", None).otherwise(col(income_col))\n",
    "        )\n",
    "        # Step 3: cast to float\n",
    "        .withColumn(\n",
    "            income_col,\n",
    "            col(income_col).cast(\"float\")\n",
    "        )\n",
    "        # Step 4: round to 2 decimals\n",
    "        .withColumn(\n",
    "            income_col,\n",
    "            round(col(income_col), 2)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return df\n",
    "  \n",
    "\n",
    "train_df = clean_annual_income_column(train_df)\n",
    "test_df = clean_annual_income_column(test_df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1aca0e50-48ea-42a9-a8c0-6a326e4089c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Annual Income Inconsistency Across Months\n",
    "- Each customer in the dataset has monthly records, but `Annual_Income` is a static attribute and should remain constant across all months for the same customer.\n",
    "- However, due to data quality issues, some months contain abnormally high or incorrect `Annual_Income` values, creating inconsistencies within a single customer’s timeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7379347-366b-4dd4-8e0b-6695977d8b54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr, col\n",
    "\n",
    "def replace_with_customer_median(\n",
    "    df,\n",
    "    value_col=\"Annual_Income\",\n",
    "    partition_col=\"Customer_ID\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Replaces a column's values with the median value per customer.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: compute median per customer\n",
    "    median_df = df.groupBy(partition_col).agg(\n",
    "        expr(f\"percentile_approx({value_col}, 0.5)\").alias(\"median_value\")\n",
    "    )\n",
    "\n",
    "    # Step 2: replace original values with median\n",
    "    df = (\n",
    "        df\n",
    "        .join(median_df, on=partition_col, how=\"left\")\n",
    "        .withColumn(value_col, col(\"median_value\"))\n",
    "        .drop(\"median_value\")\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "train_df = replace_with_customer_median(\n",
    "    train_df,\n",
    "    value_col=\"Annual_Income\",\n",
    "    partition_col=\"Customer_ID\"\n",
    ")\n",
    "\n",
    "test_df = replace_with_customer_median(\n",
    "    test_df,\n",
    "    value_col=\"Annual_Income\",\n",
    "    partition_col=\"Customer_ID\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe2e8cfc-6b85-43bf-84d4-2749a5f04dc4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Monthly_Inhand_Salary\n",
    "- **Data Type**: Float/Decimal (Numeric)\n",
    "- **Description**: Monthly take-home salary (net income) after deductions\n",
    "- **Example Values**: `1824.84`, `3037.99`, `12187.22`, `2612.49`, `2853.31`\n",
    "- **Unit**: Currency per month\n",
    "- **Formula Relationship**: Approximately Annual_Income / 12 (varies due to taxes/deductions)\n",
    "- **Purpose**: Monthly cash flow assessment and EMI eligibility\n",
    "- **Data Quality Issues**: Some missing/empty values in the dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60279f30-27ed-4ad1-9122-a54f8a783a8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Replacing Inconsistent Monthly_Inhand_Salary Values\n",
    "- We are going to use the `Customer_ID` and `month` columns to impute the `Monthly_Inhand_Salary` column. Since `Customer_ID` contains records for each month, we will leverage this information to replace inconsistent monthly inhand salary values and fill null using `forward fill` and `backward fill` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acc6ba03-006d-4152-8bf7-69a8f65da66a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, last, first, round\n",
    "\n",
    "def ffill_bfill_by_customer(\n",
    "    df,\n",
    "    value_col=\"Monthly_Inhand_Salary\",\n",
    "    partition_col=\"Customer_ID\",\n",
    "    order_col=\"Month_num\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Forward-fill and backward-fill a column per customer based on time order.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    w = Window.partitionBy(partition_col).orderBy(order_col)\n",
    "\n",
    "    df = (\n",
    "        df\n",
    "        # Step 1: forward fill\n",
    "        .withColumn(\n",
    "            value_col,\n",
    "            last(value_col, ignorenulls=True).over(w)\n",
    "        )\n",
    "        # Step 2: backward fill\n",
    "        .withColumn(\n",
    "            value_col,\n",
    "            first(value_col, ignorenulls=True)\n",
    "            .over(w.rowsBetween(0, Window.unboundedFollowing))\n",
    "        )\n",
    "        # Step 3: round to 0 decimals\n",
    "        .withColumn(\n",
    "            value_col,\n",
    "            round(col(value_col))\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "train_df = ffill_bfill_by_customer(\n",
    "    train_df,\n",
    "    value_col=\"Monthly_Inhand_Salary\"\n",
    ")\n",
    "\n",
    "test_df = ffill_bfill_by_customer(\n",
    "    test_df,\n",
    "    value_col=\"Monthly_Inhand_Salary\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c2ef483-ac9c-441b-b30d-8f6d23a3f832",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Num_Bank_Accounts\n",
    "- **Data Type**: Integer (Numeric)\n",
    "- **Description**: Number of bank accounts held by the customer\n",
    "- **Example Values**: `3`, `2`, `1`, `7`\n",
    "- **Range**: 1-7 accounts typically\n",
    "- **Purpose**: Asset diversification indicator and financial engagement measure\n",
    "- **Use Case**: Customers with more accounts may indicate better financial management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84c008a1-2e5c-4562-a856-804c9bbecfae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Replacing Inconsistent Num_Bank_Accounts Values\n",
    "- We are going to use the `Customer_ID` and `month` columns to impute the `Num_Bank_Accounts` column. Since `Customer_ID` contains records for each month, we will leverage this information to replace inconsistent Num_Bank_Accounts values using `forward fill` and `backward fill` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28b0f6e8-ff1f-48cf-bc48-f1277b0b3e7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, when, last, first\n",
    "\n",
    "def clean_num_bank_accounts(\n",
    "    df,\n",
    "    bank_col=\"Num_Bank_Accounts\",\n",
    "    partition_col=\"Customer_ID\",\n",
    "    order_col=\"Month_num\",\n",
    "    drop_zero=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Cleans and imputes the Num_Bank_Accounts column.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    w = Window.partitionBy(partition_col).orderBy(order_col)\n",
    "\n",
    "    df = (\n",
    "        df\n",
    "        # Step 1: mark invalid values as NULL\n",
    "        .withColumn(\n",
    "            bank_col,\n",
    "            when(\n",
    "                (col(bank_col) < 0) | (col(bank_col) > 11),\n",
    "                None\n",
    "            ).otherwise(col(bank_col))\n",
    "        )\n",
    "        # Step 2: forward fill\n",
    "        .withColumn(\n",
    "            bank_col,\n",
    "            last(bank_col, ignorenulls=True).over(w)\n",
    "        )\n",
    "        # Step 3: backward fill\n",
    "        .withColumn(\n",
    "            bank_col,\n",
    "            first(bank_col, ignorenulls=True)\n",
    "            .over(w.rowsBetween(0, Window.unboundedFollowing))\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Step 4: optionally drop rows where value == 0\n",
    "    if drop_zero:\n",
    "        df = df.filter(col(bank_col) != 0)\n",
    "\n",
    "    return df\n",
    "\n",
    "train_df = clean_num_bank_accounts(train_df)\n",
    "test_df = clean_num_bank_accounts(test_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "096c5036-179e-48cc-bc08-a724661a625d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Num_Credit_Card\n",
    "- **Data Type**: Integer (Numeric)\n",
    "- **Description**: Number of credit cards held by the customer\n",
    "- **Example Values**: `4`, `5`\n",
    "- **Range**: Typically 1-11 cards\n",
    "- **Purpose**: Credit exposure and revolving debt assessment\n",
    "- **Data Quality Issues**: Some anomalies like `1385` (unusually high value indicating data error)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a31f683-9b75-4fa9-bf77-43e8282b059d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Replacing Anomalous Num_Credit_Card Values\n",
    "- We are going to use the `Customer_ID` and `month` columns to impute the `Num_Credit_Card` column. Since `Customer_ID` contains records for each month, we will leverage this information to replace anomalous Num_Credit_Card values using `forward fill` and `backward fill` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8cf018ac-b592-4ee6-9d22-6e958c918ebb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, when, last, first\n",
    "\n",
    "def clean_num_credit_card(\n",
    "    df,\n",
    "    credit_col=\"Num_Credit_Card\",\n",
    "    partition_col=\"Customer_ID\",\n",
    "    order_col=\"Month_num\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Cleans and imputes the Num_Credit_Card column.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    w = Window.partitionBy(partition_col).orderBy(order_col)\n",
    "\n",
    "    df = (\n",
    "        df\n",
    "        # Step 1: mark invalid values as NULL\n",
    "        .withColumn(\n",
    "            credit_col,\n",
    "            when(col(credit_col) > 11, None)\n",
    "            .otherwise(col(credit_col))\n",
    "        )\n",
    "        # Step 2: forward fill\n",
    "        .withColumn(\n",
    "            credit_col,\n",
    "            last(credit_col, ignorenulls=True).over(w)\n",
    "        )\n",
    "        # Step 3: backward fill\n",
    "        .withColumn(\n",
    "            credit_col,\n",
    "            first(credit_col, ignorenulls=True)\n",
    "            .over(w.rowsBetween(0, Window.unboundedFollowing))\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "train_df = clean_num_credit_card(train_df)\n",
    "test_df = clean_num_credit_card(test_df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48c4433e-2abb-4dbc-8954-5e8f45792030",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Interest_Rate\n",
    "- **Data Type**: Float/Decimal (Numeric)\n",
    "- **Description**: Current interest rate applicable to the customer's loans/credit\n",
    "- **Example Values**: `3`, `6`, `8`, `4`, `5`\n",
    "- **Unit**: Percentage (%)\n",
    "- **Range**: Typically 3-35%\n",
    "- **Purpose**: Cost of borrowing and credit pricing\n",
    "- **Note**: Lower rates indicate better creditworthiness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6ac42ad-1142-47cd-8ded-dba774d38f47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Replacing Anomalous Interest_Rate Values\n",
    "- We are going to use the `Customer_ID` and `month` columns to impute the `Interest_Rate` column. Since `Customer_ID` contains records for each month, we will leverage this information to replace anomalous Interest_Rate values using `forward fill` and `backward fill` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de3440e9-7662-42e2-a0fd-08a26486c604",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, when, last, first\n",
    "\n",
    "def clean_interest_rate(\n",
    "    df,\n",
    "    rate_col=\"Interest_Rate\",\n",
    "    partition_col=\"Customer_ID\",\n",
    "    order_col=\"Month_num\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Cleans and imputes the Interest_Rate column.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    w = Window.partitionBy(partition_col).orderBy(order_col)\n",
    "\n",
    "    df = (\n",
    "        df\n",
    "        # Step 1: mark invalid values as NULL\n",
    "        .withColumn(\n",
    "            rate_col,\n",
    "            when(col(rate_col) > 35, None)\n",
    "            .otherwise(col(rate_col))\n",
    "        )\n",
    "        # Step 2: forward fill\n",
    "        .withColumn(\n",
    "            rate_col,\n",
    "            last(rate_col, ignorenulls=True).over(w)\n",
    "        )\n",
    "        # Step 3: backward fill\n",
    "        .withColumn(\n",
    "            rate_col,\n",
    "            first(rate_col, ignorenulls=True)\n",
    "            .over(w.rowsBetween(0, Window.unboundedFollowing))\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "train_df = clean_interest_rate(train_df)\n",
    "test_df = clean_interest_rate(test_df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8e4d5a5-17ce-4885-a62b-b69f98f6585f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Num_of_Loan\n",
    "- **Data Type**: Integer (Numeric)\n",
    "- **Description**: Total number of active loans the customer has\n",
    "- **Example Values**: `4`, `1`, `3`, `0`, `-100`, `967`\n",
    "- **Range**: 0-4 typical (negative and extremely high values indicate data anomalies)\n",
    "- **Purpose**: Debt burden assessment\n",
    "- **Data Quality Issues**:\n",
    "  - Negative values like `-100` are invalid\n",
    "  - Extremely high values like `967` are erroneous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "510d8b1e-3d3d-4eb8-86f6-12da7157efc4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Replacing Anomalous Num_of_Loan Values\n",
    "- We are going to use the `Customer_ID` and `month` columns to impute the `Num_of_Loan` column. Since `Customer_ID` contains records for each month, we will leverage this information to replace anomalous Num_of_Loan values using `forward fill` and `backward fill` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d965e13-14ff-4ee0-b578-226db7e78205",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, when, last, first, regexp_replace\n",
    "\n",
    "def clean_num_of_loan(\n",
    "    df,\n",
    "    loan_col=\"Num_of_Loan\",\n",
    "    partition_col=\"Customer_ID\",\n",
    "    order_col=\"Month_num\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Cleans and imputes the Num_of_Loan column.\n",
    "    \"\"\"\n",
    "\n",
    "    w = Window.partitionBy(partition_col).orderBy(order_col)\n",
    "\n",
    "    df = (\n",
    "        df\n",
    "        # Step 1: remove \"_\" placeholder\n",
    "        .withColumn(\n",
    "            loan_col,\n",
    "            regexp_replace(col(loan_col), \"_\", \"\")\n",
    "        )\n",
    "        # Step 2: empty string → NULL\n",
    "        .withColumn(\n",
    "            loan_col,\n",
    "            when(col(loan_col) == \"\", None).otherwise(col(loan_col))\n",
    "        )\n",
    "        # Step 3: cast to int\n",
    "        .withColumn(\n",
    "            loan_col,\n",
    "            col(loan_col).cast(\"int\")\n",
    "        )\n",
    "        # Step 4: mark invalid values as NULL\n",
    "        .withColumn(\n",
    "            loan_col,\n",
    "            when(\n",
    "                (col(loan_col) < 0) | (col(loan_col) > 9),\n",
    "                None\n",
    "            ).otherwise(col(loan_col))\n",
    "        )\n",
    "        # Step 5: forward fill\n",
    "        .withColumn(\n",
    "            loan_col,\n",
    "            last(loan_col, ignorenulls=True).over(w)\n",
    "        )\n",
    "        # Step 6: backward fill\n",
    "        .withColumn(\n",
    "            loan_col,\n",
    "            first(loan_col, ignorenulls=True)\n",
    "            .over(w.rowsBetween(0, Window.unboundedFollowing))\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "train_df = clean_num_of_loan(train_df)\n",
    "test_df = clean_num_of_loan(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d8a4da9-9c4a-4f0f-bfe7-9a8888c1366c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Type_of_Loan\n",
    "- **Data Type**: String (Categorical/Text)\n",
    "- **Description**: Types of loans held by the customer (can be multiple)\n",
    "- **Example Values**: \n",
    "  - `Auto Loan, Credit-Builder Loan, Personal Loan, and Home Equity Loan`\n",
    "  - `Credit-Builder Loan`\n",
    "  - `Not Specified`\n",
    "  - (Empty/blank values)\n",
    "- **Delimiter**: Comma-separated list\n",
    "- **Purpose**: Loan portfolio composition analysis\n",
    "- **Data Quality Issues**: \n",
    "  - Missing values (empty cells)\n",
    "  - Some records show \"Not Specified\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f5ba031-e6c9-4348-8414-ca83b463e99c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Standardize Type of Loan Values in Dataframes"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, regexp_replace, when, lower, coalesce, lit\n",
    "\n",
    "def clean_type_of_loan(\n",
    "    df,\n",
    "    loan_col=\"Type_of_Loan\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Cleans Type_of_Loan \n",
    "    \"\"\"\n",
    "\n",
    "    #  clean raw column\n",
    "    \n",
    "    df = (\n",
    "        df\n",
    "        #replace \"Null\" with \"UnSpecified\" .\n",
    "        .fillna( value = \"UnSpecified\", subset = [loan_col] )\n",
    "\n",
    "        # replace \" and \" with space\n",
    "        .withColumn(\n",
    "            loan_col,\n",
    "            regexp_replace(col(loan_col), \" and \", \" \")\n",
    "        )\n",
    "        # normalize invalid strings to NULL\n",
    "        .withColumn(\n",
    "            loan_col,\n",
    "            when(\n",
    "                (col(loan_col) == \"nan\") | (col(loan_col) == \"Not Specified\"),\n",
    "                None\n",
    "            ).otherwise(col(loan_col))\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "    return df\n",
    "\n",
    "train_df = clean_type_of_loan(train_df)\n",
    "test_df = clean_type_of_loan(test_df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8235bef7-387e-49d5-9b2b-d2f832379797",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Num_of_Delayed_Payment\n",
    "- **Data Type**: Integer (Numeric)\n",
    "- **Description**: Total number of times the customer has made delayed payments\n",
    "- **Example Values**: `7`, `4`, `8`, `1`, `6`, `9`\n",
    "- **Range**: Typically 0-9 instances, but in this dataset range is 0-28\n",
    "- **Purpose**: Payment delinquency history\n",
    "- **Data Quality Issues**: \n",
    "  - Some missing values\n",
    "  - Some values have trailing underscore like `1295_`\n",
    "  - Some anomalies like `2820` (unusually high value indicating data error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a09aaf41-d329-4082-b8ae-d8448d3b6610",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Replacing Anomalous Num_of_Delayed_Payment Values\n",
    "- We are going to use the `Customer_ID` and `month` columns to impute the `Num_of_Delayed_Payment` column. Since `Customer_ID` contains records for each month, we will leverage this information to replace anomalous Num_of_Delayed_Payment values and fill null using `forward fill` and `backward fill` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52838742-22af-41c2-94cd-25aaef313059",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, when, last, first, regexp_replace\n",
    "\n",
    "def clean_num_of_delayed_payment(\n",
    "    df,\n",
    "    delayed_col=\"Num_of_Delayed_Payment\",\n",
    "    partition_col=\"Customer_ID\",\n",
    "    order_col=\"Month_num\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Cleans and imputes the Num_of_Delayed_Payment column.\n",
    "    \"\"\"\n",
    "\n",
    "    w = Window.partitionBy(partition_col).orderBy(order_col)\n",
    "\n",
    "    df = (\n",
    "        df\n",
    "        # Step 1: remove \"_\" placeholder\n",
    "        .withColumn(\n",
    "            delayed_col,\n",
    "            regexp_replace(col(delayed_col), \"_\", \"\")\n",
    "        )\n",
    "        # Step 2: empty string → NULL\n",
    "        .withColumn(\n",
    "            delayed_col,\n",
    "            when(col(delayed_col) == \"\", None).otherwise(col(delayed_col))\n",
    "        )\n",
    "        # Step 3: cast to int\n",
    "        .withColumn(\n",
    "            delayed_col,\n",
    "            col(delayed_col).cast(\"int\")\n",
    "        )\n",
    "        # Step 4: mark invalid values as NULL\n",
    "        .withColumn(\n",
    "            delayed_col,\n",
    "            when(\n",
    "                (col(delayed_col) < 0) | (col(delayed_col) > 27),\n",
    "                None\n",
    "            ).otherwise(col(delayed_col))\n",
    "        )\n",
    "        # Step 5: forward fill\n",
    "        .withColumn(\n",
    "            delayed_col,\n",
    "            last(delayed_col, ignorenulls=True).over(w)\n",
    "        )\n",
    "        # Step 6: backward fill\n",
    "        .withColumn(\n",
    "            delayed_col,\n",
    "            first(delayed_col, ignorenulls=True)\n",
    "            .over(w.rowsBetween(0, Window.unboundedFollowing))\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "train_df = clean_num_of_delayed_payment(train_df)\n",
    "test_df = clean_num_of_delayed_payment(test_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "258a4b49-b2c2-4ef3-92c2-fa97dbd67a11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Changed_Credit_Limit\n",
    "- **Data Type**: Float/Decimal (Numeric)\n",
    "- **Description**: Recent changes in the customer's credit limit (in currency units)\n",
    "- **Example Values**: `11.27`, `6.27`, `9.27`, `7.1`, `5.42`\n",
    "- **Unit**: Currency or percentage points\n",
    "- **Purpose**: Credit limit adjustments and credit portfolio changes\n",
    "- **Note**: Can be positive (increase) or negative (decrease)\n",
    "- **Data Quality Issues**: some rows contain underscore only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ed55807-ea4e-4268-9a4c-e1268043c251",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Replacing Inconsistent Changed_Credit_Limit Values\n",
    "- We are going to use the `Customer_ID` and `month` columns to impute the `Changed_Credit_Limit` column. Since `Customer_ID` contains records for each month, we will leverage this information to replace `_` values using `forward fill` and `backward fill` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2683444-628b-4911-9289-c8e5c16a66c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, when, last, first, regexp_replace, round\n",
    "\n",
    "def clean_changed_credit_limit(\n",
    "    df,\n",
    "    limit_col=\"Changed_Credit_Limit\",\n",
    "    partition_col=\"Customer_ID\",\n",
    "    order_col=\"Month_num\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Cleans and imputes the Changed_Credit_Limit column.\n",
    "    \"\"\"\n",
    "\n",
    "    w = Window.partitionBy(partition_col).orderBy(order_col)\n",
    "\n",
    "    df = (\n",
    "        df\n",
    "        # Step 1: remove \"_\" placeholder\n",
    "        .withColumn(\n",
    "            limit_col,\n",
    "            regexp_replace(col(limit_col), \"_\", \"\")\n",
    "        )\n",
    "        # Step 2: empty string → NULL\n",
    "        .withColumn(\n",
    "            limit_col,\n",
    "            when(col(limit_col) == \"\", None).otherwise(col(limit_col))\n",
    "        )\n",
    "        # Step 3: cast to float\n",
    "        .withColumn(\n",
    "            limit_col,\n",
    "            col(limit_col).cast(\"float\")\n",
    "        )\n",
    "        # Step 4: forward fill\n",
    "        .withColumn(\n",
    "            limit_col,\n",
    "            last(limit_col, ignorenulls=True).over(w)\n",
    "        )\n",
    "        # Step 5: backward fill\n",
    "        .withColumn(\n",
    "            limit_col,\n",
    "            first(limit_col, ignorenulls=True)\n",
    "            .over(w.rowsBetween(0, Window.unboundedFollowing))\n",
    "        )\n",
    "        # Step 5: round to 2 decimals\n",
    "        .withColumn(\n",
    "            limit_col,\n",
    "            round(col(limit_col))\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "train_df = clean_changed_credit_limit(train_df)\n",
    "test_df = clean_changed_credit_limit(test_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50e80a3a-d149-4fb2-8313-e046fca7f050",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Num_Credit_Inquiries\n",
    "- **Data Type**: Integer (Numeric)\n",
    "- **Description**: Number of times the customer's credit was inquired\n",
    "- **Example Values**: `4`, `2`, `3`\n",
    "- **Purpose**: Credit-seeking behavior indicator\n",
    "- **Interpretation**: Higher inquiries may indicate active credit seeking or financial stress\n",
    "- **Data Quality Issues**: \n",
    "  - Some missing values\n",
    "  - Some anomalies like `2061` (unusually high value indicating data error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21d6861d-8378-4df6-af4f-0bfaacbe0f89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Replacing Anomalous Num_Credit_Inquiries Values\n",
    "- We are going to use the `Customer_ID` and `month` columns to impute the `Num_Credit_Inquiries` column. Since `Customer_ID` contains records for each month, we will leverage this information to replace anomalous Num_Credit_Inquiries values and fill null using `forward fill` and `backward fill` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e518f5ad-5327-4d76-b614-7212cf2f83aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, when, last, first\n",
    "\n",
    "def clean_num_credit_inquiries(\n",
    "    df,\n",
    "    inquiry_col=\"Num_Credit_Inquiries\",\n",
    "    partition_col=\"Customer_ID\",\n",
    "    order_col=\"Month_num\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Cleans and imputes the Num_Credit_Inquiries column.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    w = Window.partitionBy(partition_col).orderBy(order_col)\n",
    "\n",
    "    df = (\n",
    "        df\n",
    "        # Step 1: mark invalid values as NULL\n",
    "        .withColumn(\n",
    "            inquiry_col,\n",
    "            when(col(inquiry_col) > 18, None)\n",
    "            .otherwise(col(inquiry_col))\n",
    "        )\n",
    "        # Step 2: forward fill\n",
    "        .withColumn(\n",
    "            inquiry_col,\n",
    "            last(inquiry_col, ignorenulls=True).over(w)\n",
    "        )\n",
    "        # Step 3: backward fill\n",
    "        .withColumn(\n",
    "            inquiry_col,\n",
    "            first(inquiry_col, ignorenulls=True)\n",
    "            .over(w.rowsBetween(0, Window.unboundedFollowing))\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "train_df = clean_num_credit_inquiries(train_df)\n",
    "test_df = clean_num_credit_inquiries(test_df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "013a8509-44b2-48ab-8d2a-cb631819bae4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Credit_Mix\n",
    "- **Data Type**: String (Categorical)\n",
    "- **Description**: Diversity of credit types held by the customer\n",
    "- **Example Values**: `Good`, `Standard`, `_` (underscore = missing)\n",
    "- **Categories**: \n",
    "  - `Good`: Mix of secured and unsecured credit\n",
    "  - `Standard`: Limited credit diversity\n",
    "  - `_` or blank: Missing data\n",
    "- **Purpose**: Credit portfolio health assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5dc3d5b5-e62c-419e-9c95-f48e46fa38b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Replacing Inconsistent Credit_Mix Values\n",
    "- We are going to use the `Customer_ID` and `month` columns to impute the `Credit_Mix` column. Since `Customer_ID` contains records for each month, we will leverage this information to replace `_` values using `forward fill` and `backward fill` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f301ebc-588b-4112-9384-6167ecdd0508",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, when, last, first, regexp_replace, lower\n",
    "\n",
    "def clean_credit_mix(\n",
    "    df,\n",
    "    credit_mix_col=\"Credit_Mix\",\n",
    "    partition_col=\"Customer_ID\",\n",
    "    order_col=\"Month_num\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Cleans and imputes the Credit_Mix column.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    w = Window.partitionBy(partition_col).orderBy(order_col)\n",
    "\n",
    "    df = (\n",
    "        df\n",
    "        # Step 1: remove \"_\" placeholder\n",
    "        .withColumn(\n",
    "            credit_mix_col,\n",
    "            regexp_replace(col(credit_mix_col), \"_\", \"\")\n",
    "        )\n",
    "        # Step 2: empty string → NULL\n",
    "        .withColumn(\n",
    "            credit_mix_col,\n",
    "            when(col(credit_mix_col) == \"\", None)\n",
    "            .otherwise(col(credit_mix_col))\n",
    "        )\n",
    "        # Step 3: forward fill\n",
    "        .withColumn(\n",
    "            credit_mix_col,\n",
    "            last(credit_mix_col, ignorenulls=True).over(w)\n",
    "        )\n",
    "        # Step 4: backward fill\n",
    "        .withColumn(\n",
    "            credit_mix_col,\n",
    "            first(credit_mix_col, ignorenulls=True)\n",
    "            .over(w.rowsBetween(0, Window.unboundedFollowing))\n",
    "        )\n",
    "        # Step 5: standardize case\n",
    "        .withColumn(\n",
    "            credit_mix_col,\n",
    "            lower(col(credit_mix_col))\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "train_df = clean_credit_mix(train_df)\n",
    "test_df = clean_credit_mix(test_df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99fea449-4fd5-4d97-a164-a87f7e59c614",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Outstanding_Debt\n",
    "- **Data Type**: Float/Decimal (Numeric)\n",
    "- **Description**: Total outstanding debt amount owed by the customer\n",
    "- **Example Values**: `809.98`, `605.03`, `1303.01`, `632.46`, `943.86`\n",
    "- **Unit**: Currency\n",
    "- **Purpose**: Total debt burden and solvency assessment\n",
    "- **Risk Factor**: Higher outstanding debt = higher default risk\n",
    "- **Data Quality Issues**: some rows contain trailing underscore like `706.22_`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f8d223d-c096-47ef-b1db-1b8a89b7732d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace, col, when, round\n",
    "\n",
    "def clean_outstanding_debt(\n",
    "    df,\n",
    "    debt_col=\"Outstanding_Debt\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Cleans the Outstanding_Debt column.\n",
    "    \"\"\"\n",
    "\n",
    "    df = (\n",
    "        df\n",
    "        # Step 1: remove \"_\" placeholder\n",
    "        .withColumn(\n",
    "            debt_col,\n",
    "            regexp_replace(col(debt_col), \"_\", \"\")\n",
    "        )\n",
    "        # Step 2: empty string → NULL\n",
    "        .withColumn(\n",
    "            debt_col,\n",
    "            when(col(debt_col) == \"\", None).otherwise(col(debt_col))\n",
    "        )\n",
    "        # Step 3: cast to float\n",
    "        .withColumn(\n",
    "            debt_col,\n",
    "            col(debt_col).cast(\"float\")\n",
    "        )\n",
    "        # Step 4: round to 0 decimals\n",
    "        .withColumn(\n",
    "            debt_col,\n",
    "            round(col(debt_col))\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "train_df = clean_outstanding_debt(train_df)\n",
    "test_df = clean_outstanding_debt(test_df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e80c19ec-682c-40fb-8657-a92246f20a26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Credit_History_Age\n",
    "- **Data Type**: String (Duration/Time Period)\n",
    "- **Description**: Length of the customer's credit history\n",
    "- **Example Values**: \n",
    "  - `22 Years and 1 Months`\n",
    "  - `26 Years and 7 Months`\n",
    "  - `17 Years and 9 Months`\n",
    "  - `NA` (missing data)\n",
    "- **Unit**: Years and Months\n",
    "- **Purpose**: Credit experience and stability assessment\n",
    "- **Data Quality Issues**: Some records show `NA` for missing values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94e832ce-2738-4264-96c9-99cb34587fe5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Converting Credit_History_Age  into total months\n",
    "- We are going to use the `Customer_ID` and `month` columns to impute the `Credit_History_Age` column. Since `Customer_ID` contains records for each month, we will leverage this information to fill `NA` values using `forward fill` and `backward fill` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d86feb96-05f2-43c7-99d1-0b5820bd24e6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Convert Credit_History_Age to total months"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, regexp_extract\n",
    "\n",
    "def parse_credit_history_age(\n",
    "    df,\n",
    "    age_col=\"Credit_History_Age\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Converts 'X Years and Y Months' into total months.\n",
    "    Handles 'NA' and NULL safely.\n",
    "    \"\"\"\n",
    "\n",
    "    df = df.withColumn(\n",
    "        age_col,\n",
    "        when(\n",
    "            (col(age_col) == \"NA\") | col(age_col).isNull(),\n",
    "            None\n",
    "        ).otherwise(\n",
    "            regexp_extract(col(age_col), r\"(\\d+)\\s+Years\", 1).cast(\"int\") * 12 +\n",
    "            regexp_extract(col(age_col), r\"(\\d+)\\s+Months\", 1).cast(\"int\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "train_df = parse_credit_history_age(train_df)\n",
    "test_df = parse_credit_history_age(test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45131e52-13d6-4fbe-bfe5-2083527a9dc0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, last, first, when, row_number\n",
    "\n",
    "def sequential_impute(\n",
    "    df,\n",
    "    value_col,\n",
    "    partition_col=\"Customer_ID\",\n",
    "    order_col=\"Month_num\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Sequentially imputes missing values in a monotonically increasing\n",
    "    monthly column (e.g., Credit_History_Age).\n",
    "\n",
    "    Rules:\n",
    "    - Missing AFTER a known value → last_value + distance\n",
    "    - Missing BEFORE a known value → next_value - distance\n",
    "    \"\"\"\n",
    "\n",
    "    w = Window.partitionBy(partition_col).orderBy(order_col)\n",
    "\n",
    "    # Row number for distance calculation\n",
    "    df = df.withColumn(\"rn\", row_number().over(w))\n",
    "\n",
    "    # Last known value (forward reference)\n",
    "    df = df.withColumn(\n",
    "        \"last_val\",\n",
    "        last(value_col, ignorenulls=True).over(w)\n",
    "    )\n",
    "\n",
    "    # Row number of last known value\n",
    "    df = df.withColumn(\n",
    "        \"last_val_rn\",\n",
    "        when(col(value_col).isNotNull(), col(\"rn\"))\n",
    "    )\n",
    "\n",
    "    df = df.withColumn(\n",
    "        \"last_val_rn\",\n",
    "        last(\"last_val_rn\", ignorenulls=True).over(w)\n",
    "    )\n",
    "\n",
    "    # Next known value (backward reference)\n",
    "    df = df.withColumn(\n",
    "        \"next_val\",\n",
    "        first(value_col, ignorenulls=True)\n",
    "        .over(w.rowsBetween(0, Window.unboundedFollowing))\n",
    "    )\n",
    "\n",
    "    # Row number of next known value\n",
    "    df = df.withColumn(\n",
    "        \"next_val_rn\",\n",
    "        when(col(value_col).isNotNull(), col(\"rn\"))\n",
    "    )\n",
    "\n",
    "    df = df.withColumn(\n",
    "        \"next_val_rn\",\n",
    "        first(\"next_val_rn\", ignorenulls=True)\n",
    "        .over(w.rowsBetween(0, Window.unboundedFollowing))\n",
    "    )\n",
    "\n",
    "    # Apply sequential logic\n",
    "    df = df.withColumn(\n",
    "        value_col,\n",
    "        when(col(value_col).isNotNull(), col(value_col))\n",
    "        .when(\n",
    "            col(\"last_val\").isNotNull(),\n",
    "            col(\"last_val\") + (col(\"rn\") - col(\"last_val_rn\"))\n",
    "        )\n",
    "        .when(\n",
    "            col(\"next_val\").isNotNull(),\n",
    "            col(\"next_val\") - (col(\"next_val_rn\") - col(\"rn\"))\n",
    "        )\n",
    "        .otherwise(None)\n",
    "    )\n",
    "\n",
    "    # Cleanup helper columns\n",
    "    df = df.drop(\n",
    "        \"rn\",\n",
    "        \"last_val\",\n",
    "        \"last_val_rn\",\n",
    "        \"next_val\",\n",
    "        \"next_val_rn\"\n",
    "    )\n",
    "\n",
    "    return df\n",
    "train_df = sequential_impute(\n",
    "    train_df,\n",
    "    value_col=\"Credit_History_Age\",\n",
    "    partition_col=\"Customer_ID\",\n",
    "    order_col=\"Month_num\"\n",
    ")\n",
    "\n",
    "\n",
    "test_df = sequential_impute(\n",
    "    test_df,\n",
    "    value_col=\"Credit_History_Age\",\n",
    "    partition_col=\"Customer_ID\",\n",
    "    order_col=\"Month_num\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6efc9957-6dc7-4489-9bc8-f8e75fe0405e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Total_EMI_per_month\n",
    "- **Data Type**: Float/Decimal (Numeric)\n",
    "- **Description**: Total Equated Monthly Installment (EMI) amount across all loans\n",
    "- **Example Values**: `49.57`, `18.82`, `246.99`, `16.42`, `0`\n",
    "- **Unit**: Currency per month\n",
    "- **Formula**: Sum of all monthly loan installments\n",
    "- **Purpose**: Monthly obligation assessment\n",
    "- **Note**: `0` indicates no active loan EMI\n",
    "- **Data Quality Issues**: \n",
    "  - Some anomalies like `82238` (unusually high value indicating data error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f559e9df-c56c-42b5-b880-6a3f808382d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Total_EMI_per_month Anomalous Across Months\n",
    "- Each customer in the dataset has monthly records, but `Total_EMI_per_month` is a static attribute and should remain constant across all months for the same customer.\n",
    "- However, due to data quality issues, some months contain abnormally high or incorrect `Total_EMI_per_month` values, creating inconsistencies within a single customer’s timeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8528448a-db92-493a-9aa5-5a95c40ce64a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr, col, round\n",
    "\n",
    "def replace_with_customer_median(\n",
    "    df,\n",
    "    value_col=\"Total_EMI_per_month\",\n",
    "    partition_col=\"Customer_ID\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Replaces a column's values with the median value per customer.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: compute median per customer\n",
    "    median_df = df.groupBy(partition_col).agg(\n",
    "        expr(f\"percentile_approx({value_col}, 0.5)\").alias(\"median_value\")\n",
    "    )\n",
    "\n",
    "    # Step 2: replace original values with median\n",
    "    df = (\n",
    "        df\n",
    "        .join(median_df, on=partition_col, how=\"left\")\n",
    "        .withColumn(value_col, round(col(\"median_value\")))\n",
    "        .drop(\"median_value\")\n",
    "\n",
    "    )\n",
    "\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "train_df = replace_with_customer_median(train_df, \"Total_EMI_per_month\")\n",
    "test_df  = replace_with_customer_median(test_df, \"Total_EMI_per_month\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74e03ce6-a700-442b-8d3a-be9a23023e32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Amount_invested_monthly\n",
    "- **Data Type**: Convert into Float/Decimal (Numeric)\n",
    "- **Description**: Amount the customer invests or saves monthly\n",
    "- **Example Values**: `80.42`, `118.28`, `104.29`, `81.70`, `276.73`\n",
    "- **Unit**: Currency per month\n",
    "- **Purpose**: Savings behavior and financial capacity assessment\n",
    "- **Data Quality Issues**: \n",
    "  - Some missing values\n",
    "  - Some anomalies like `__10000__` (formatting errors)\n",
    "  - Values with underscores indicate corruption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5363add-87b2-4bf1-ae35-6fb440541c13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Replacing Inconsistent Amount_invested_monthly\n",
    "- We are going to use the `Customer_ID` and `month` columns to impute the `Amount_invested_monthly` column. Since `Customer_ID` contains records for each month, we will leverage this information to replace `__10000__` and fill null values using `forward fill` and `backward fill` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ffb5fc9-f8a3-4da5-906a-6d67d90e904b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, when, last, first, round\n",
    "\n",
    "def clean_amount_invested_monthly(\n",
    "    df,\n",
    "    invest_col=\"Amount_invested_monthly\",\n",
    "    partition_col=\"Customer_ID\",\n",
    "    order_col=\"Month_num\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Cleans and imputes the Amount_invested_monthly column.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    w = Window.partitionBy(partition_col).orderBy(order_col)\n",
    "\n",
    "    df = (\n",
    "        df\n",
    "        # Step 1: mark invalid placeholder as NULL\n",
    "        .withColumn(\n",
    "            invest_col,\n",
    "            when(col(invest_col) == \"__10000__\", None)\n",
    "            .otherwise(col(invest_col))\n",
    "        )\n",
    "        # Step 2: cast to float\n",
    "        .withColumn(\n",
    "            invest_col,\n",
    "            col(invest_col).cast(\"float\")\n",
    "        )\n",
    "        # Step 3: forward fill\n",
    "        .withColumn(\n",
    "            invest_col,\n",
    "            last(invest_col, ignorenulls=True).over(w)\n",
    "        )\n",
    "        # Step 4: backward fill\n",
    "        .withColumn(\n",
    "            invest_col,\n",
    "            first(invest_col, ignorenulls=True)\n",
    "            .over(w.rowsBetween(0, Window.unboundedFollowing))\n",
    "        )\n",
    "        # Step 5: round to 0 decimals\n",
    "        .withColumn(\n",
    "            invest_col,\n",
    "            round(col(invest_col))\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "train_df = clean_amount_invested_monthly(train_df)\n",
    "test_df = clean_amount_invested_monthly(test_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec8c40a3-829c-4608-8538-fd33c79b97d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Payment_Behaviour\n",
    "- **Data Type**: String (Categorical)\n",
    "- **Description**: Pattern or category of payment behavior\n",
    "- **Example Values**: \n",
    "  - `High_spent_Small_value_payments`\n",
    "  - `Low_spent_Large_value_payments`\n",
    "  - `Low_spent_Medium_value_payments`\n",
    "  - `High_spent_Medium_value_payments`\n",
    "  - `!@9#%8` (corrupted data)\n",
    "- **Categories**: Combination of spending level (High/Low) and payment value size\n",
    "- **Purpose**: Behavioral segmentation and credit scoring\n",
    "- **Data Quality Issues**: Some corrupted values with special characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b63afbb8-b95e-457a-ac0a-878c2a59b744",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Replacing Inconsistent Payment_Behaviour\n",
    "- We are going to use the `Customer_ID` and `month` columns to impute the `Payment_Behaviour` column. Since `Customer_ID` contains records for each month, we will leverage this information to replace `!@9#%8` values using `forward fill` and `backward fill` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10b47295-d7b1-45cb-946d-25aa2993dee6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, when, last, first, lower\n",
    "\n",
    "def clean_payment_behaviour(\n",
    "    df,\n",
    "    behaviour_col=\"Payment_Behaviour\",\n",
    "    partition_col=\"Customer_ID\",\n",
    "    order_col=\"Month_num\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Cleans and imputes the Payment_Behaviour column.\n",
    "    \"\"\"\n",
    "\n",
    "    w = Window.partitionBy(partition_col).orderBy(order_col)\n",
    "\n",
    "    df = (\n",
    "        df\n",
    "        # Step 1: mark invalid placeholder as NULL\n",
    "        .withColumn(\n",
    "            behaviour_col,\n",
    "            when(col(behaviour_col) == \"!@9#%8\", None)\n",
    "            .otherwise(col(behaviour_col))\n",
    "        )\n",
    "        # Step 2: forward fill\n",
    "        .withColumn(\n",
    "            behaviour_col,\n",
    "            last(behaviour_col, ignorenulls=True).over(w)\n",
    "        )\n",
    "        # Step 3: backward fill\n",
    "        .withColumn(\n",
    "            behaviour_col,\n",
    "            first(behaviour_col, ignorenulls=True)\n",
    "            .over(w.rowsBetween(0, Window.unboundedFollowing))\n",
    "        )\n",
    "        # Step 4: normalize case\n",
    "        .withColumn(\n",
    "            behaviour_col,\n",
    "            lower(col(behaviour_col))\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "train_df = clean_payment_behaviour(train_df)\n",
    "test_df = clean_payment_behaviour(test_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66540867-d4a6-49b2-919c-0f3a2533edcd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Monthly_Balance\n",
    "- **Data Type**: Float/Decimal (Numeric)\n",
    "- **Description**: Monthly account balance after all transactions and payments\n",
    "- **Example Values**: `312.49`, `284.63`, `331.21`, `340.48`, `288.61`\n",
    "- **Unit**: Currency\n",
    "- **Purpose**: Liquidity position and cash flow assessment\n",
    "- **Interpretation**: Higher balance indicates better financial health\n",
    "- **Data Quality Issues**: \n",
    "  - Some corrupted values with `__-333333333333333333333333333__`\n",
    "  - Some missing values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "066cefb5-0b90-4988-a6a9-a9e500ae3be3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Replacing Inconsistent Monthly_Balance\n",
    "- We are going to use the `Customer_ID` and `month` columns to impute the `Monthly_Balance` column. Since `Customer_ID` contains records for each month, we will leverage this information to replace `__-333333333333333333333333333__` and fill null values using `forward fill` and `backward fill` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f862ecd3-725f-4fc5-9452-f2977a3ede1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, when, last, first, regexp_replace, round\n",
    "\n",
    "def clean_monthly_balance(\n",
    "    df,\n",
    "    balance_col=\"Monthly_Balance\",\n",
    "    partition_col=\"Customer_ID\",\n",
    "    order_col=\"Month_num\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Cleans and imputes the Monthly_Balance column.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    w = Window.partitionBy(partition_col).orderBy(order_col)\n",
    "\n",
    "    df = (\n",
    "        df\n",
    "        # Step 1: remove invalid placeholder\n",
    "        .withColumn(\n",
    "            balance_col,\n",
    "            regexp_replace(\n",
    "                col(balance_col),\n",
    "                \"__-333333333333333333333333333__\",\n",
    "                \"\"\n",
    "            )\n",
    "        )\n",
    "        # Step 2: empty string → NULL\n",
    "        .withColumn(\n",
    "            balance_col,\n",
    "            when(col(balance_col) == \"\", None)\n",
    "            .otherwise(col(balance_col))\n",
    "        )\n",
    "        # Step 3: cast to float\n",
    "        .withColumn(\n",
    "            balance_col,\n",
    "            col(balance_col).cast(\"float\")\n",
    "        )\n",
    "        # Step 4: forward fill\n",
    "        .withColumn(\n",
    "            balance_col,\n",
    "            last(balance_col, ignorenulls=True).over(w)\n",
    "        )\n",
    "        # Step 5: backward fill\n",
    "        .withColumn(\n",
    "            balance_col,\n",
    "            first(balance_col, ignorenulls=True)\n",
    "            .over(w.rowsBetween(0, Window.unboundedFollowing))\n",
    "        )\n",
    "        # Step 6: round to 0 decimals\n",
    "        .withColumn(\n",
    "            balance_col,\n",
    "            round(col(balance_col))\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "train_df = clean_monthly_balance(train_df)\n",
    "test_df = clean_monthly_balance(test_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75719e5f-d23c-4f96-858e-8df11eea119f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Credit_Score\n",
    "- **Data Type**: String (Categorical)\n",
    "- **Description**: Credit score category/rating of the customer\n",
    "- **Example Values**: `Good`, `Standard`, `Poor` (if present)\n",
    "- **Categories**: \n",
    "  - `Good`: Creditworthy (typically 700+)\n",
    "  - `Standard`: Average credit (600-700)\n",
    "  - `Poor`: Low creditworthiness (<600)\n",
    "- **Purpose**: Overall credit rating for loan/credit approval decisions\n",
    "- **Use Case**: Primary target variable in credit risk models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20bf29e6-9359-401a-83db-fdc40c809fd2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lower, trim\n",
    "\n",
    "def clean_credit_score(\n",
    "    df,\n",
    "    credit_score_col=\"Credit_Score\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Standardizes the Credit_Score column.\n",
    "    \"\"\"\n",
    "\n",
    "    df = df.withColumn(\n",
    "        credit_score_col,\n",
    "        trim(lower(col(credit_score_col)))\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "train_df = clean_credit_score(train_df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5ad28cd-6cf8-4d3c-b250-18eed3442c82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Payment_of_Min_Amount\n",
    "- **Data Type**: String (Categorical - Boolean-like)\n",
    "- **Description**: Whether the customer pays at least the minimum amount due\n",
    "- **Example Values**: `Yes`, `No`, `NM` (likely \"Not Mentioned\" or missing)\n",
    "- **Categories**: \n",
    "  - `Yes`: Pays minimum amount regularly\n",
    "  - `No`: Fails to pay minimum amount\n",
    "  - `NM`: Data missing or not mentioned\n",
    "- **Purpose**: Minimum payment compliance indicator\n",
    "- **Risk Assessment**: `No` indicates higher default risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8eaebfd-3dc9-4eb8-8ee5-37d3cf255750",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lower, trim\n",
    "\n",
    "def clean_payment_of_min_amount(\n",
    "    df,\n",
    "    payment_col=\"Payment_of_Min_Amount\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Standardizes the Payment_of_Min_Amount column.\n",
    "    \"\"\"\n",
    "\n",
    "    df = df.withColumn(\n",
    "        payment_col,\n",
    "        trim(lower(col(payment_col)))\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "train_df = clean_payment_of_min_amount(train_df)\n",
    "test_df = clean_payment_of_min_amount(test_df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95e599ae-7c7f-4b6c-8d7f-08696d350731",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da08269b-18d1-4df2-9152-5ae15ad8a91c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Creating Income_Group column based on GNI per capita thresholds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8733d2d-bdba-4586-a378-2d719ddbed17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "def add_income_group(\n",
    "    df,\n",
    "    income_col=\"Monthly_Inhand_Salary\",\n",
    "    output_col=\"Income_Group\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Creates Income_Group column based on GNI per capita thresholds.\n",
    "\n",
    "    Groups:\n",
    "    - Low income: ≤ 1,135\n",
    "    - Lower-middle income: 1,136 - 4,495\n",
    "    - Upper-middle income: 4,496 - 13,935\n",
    "    - High income: > 13,935\n",
    "    \"\"\"\n",
    "\n",
    "    df = df.withColumn(\n",
    "        output_col,\n",
    "        when(col(income_col).isNull(), None)\n",
    "        .when(col(income_col) <= 1135, \"low_income\")\n",
    "        .when((col(income_col) >= 1136) & (col(income_col) <= 4495), \"lower_middle_income\")\n",
    "        .when((col(income_col) >= 4496) & (col(income_col) <= 13935), \"upper_middle_income\")\n",
    "        .otherwise(\"high_income\")\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "train_df = add_income_group(train_df)\n",
    "test_df  = add_income_group(test_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fbad56e5-bd4f-4a08-9711-e112ccfa5ab3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Transform 'Type_of_Loan' into individual feature columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a731413-f9c0-4784-bd1f-01baae6a25cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_loan_type_flags(\n",
    "    df,\n",
    "    loan_col=\"Type_of_Loan\"\n",
    "):\n",
    "    \"\"\"\n",
    "    creates binary indicator columns\n",
    "    for each known loan type.\n",
    "    \"\"\"\n",
    "     # Step 1: lowercase + NULL-safe column for parsing\n",
    "    df = df.withColumn(\n",
    "        \"loan_lc\",\n",
    "        lower(coalesce(col(loan_col), lit(\"\")))\n",
    "    )\n",
    "\n",
    "    # Step 2: create binary columns\n",
    "    df = (\n",
    "        df\n",
    "        .withColumn(\"has_auto_loan\",\n",
    "                    col(\"loan_lc\").contains(\"auto loan\").cast(\"int\"))\n",
    "        .withColumn(\"has_credit_builder_loan\",\n",
    "                    col(\"loan_lc\").contains(\"credit-builder loan\").cast(\"int\"))\n",
    "        .withColumn(\"has_debt_consolidation_loan\",\n",
    "                    col(\"loan_lc\").contains(\"debt consolidation loan\").cast(\"int\"))\n",
    "        .withColumn(\"has_home_equity_loan\",\n",
    "                    col(\"loan_lc\").contains(\"home equity loan\").cast(\"int\"))\n",
    "        .withColumn(\"has_mortgage_loan\",\n",
    "                    col(\"loan_lc\").contains(\"mortgage loan\").cast(\"int\"))\n",
    "        .withColumn(\"has_payday_loan\",\n",
    "                    col(\"loan_lc\").contains(\"payday loan\").cast(\"int\"))\n",
    "        .withColumn(\"has_personal_loan\",\n",
    "                    col(\"loan_lc\").contains(\"personal loan\").cast(\"int\"))\n",
    "        .withColumn(\"has_student_loan\",\n",
    "                    col(\"loan_lc\").contains(\"student loan\").cast(\"int\"))\n",
    "    )\n",
    "\n",
    "    # Step 3: drop helper & raw column\n",
    "    df = df.drop(\"loan_lc\", loan_col)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "train_df = create_loan_type_flags(train_df, \"Type_of_Loan\")\n",
    "test_df = create_loan_type_flags(test_df, \"Type_of_Loan\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5ff3498-134b-4dd9-8dfd-86d61f7b2a69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Payment_Behaviour\n",
    "- Values follow a clear pattern like:\n",
    "   - High_spent_Small_value_payments\n",
    "   - Low_spent_Large_value_payments\n",
    "   - Low_spent_Medium_value_payments\n",
    "   - High_spent_Medium_value_payments\n",
    "\n",
    "- This encodes two concepts in one string:\n",
    "  - Spending level: `High / Low`\n",
    "  - Typical transaction size: `Small / Medium / Large`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e239d349-527e-4952-814b-1783ea11aa70",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "def split_payment_behaviour(\n",
    "    df,\n",
    "    source_col=\"Payment_Behaviour\",\n",
    "    spending_col=\"spending_behavior\",\n",
    "    value_col=\"payment_value\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Splits payment behaviour into two numeric features:\n",
    "    1. Spending behavior: Low_spent -> 0, High_spent -> 1\n",
    "    2. Payment value: Small -> 0, Medium -> 1, Large -> 2\n",
    "    \"\"\"\n",
    "\n",
    "    df = (\n",
    "        df\n",
    "        # Spending behaviour\n",
    "        .withColumn(\n",
    "            spending_col,\n",
    "            when(col(source_col).isNull(), None)\n",
    "            .when(col(source_col).contains(\"Low_spent\"), 0)\n",
    "            .otherwise(1)\n",
    "        )\n",
    "        # Payment value\n",
    "        .withColumn(\n",
    "            value_col,\n",
    "            when(col(source_col).isNull(), None)\n",
    "            .when(col(source_col).contains(\"Small_value\"), 0)\n",
    "            .when(col(source_col).contains(\"Medium_value\"), 1)\n",
    "            .otherwise(2)  # Large_value\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "train_df = split_payment_behaviour(\n",
    "    train_df,\n",
    "    source_col=\"Payment_Behaviour\",\n",
    "    spending_col=\"spend_level\",\n",
    "    value_col=\"txn_value_level\"\n",
    ")\n",
    "\n",
    "\n",
    "test_df = split_payment_behaviour(\n",
    "    test_df,\n",
    "    source_col=\"Payment_Behaviour\",\n",
    "    spending_col=\"spend_level\",\n",
    "    value_col=\"txn_value_level\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2fc3a2b-4dbc-4724-8147-64d309e2621c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Creating Debt_to_Income_Ratio Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28d7e60b-6ddb-41f0-86d6-9e13de18382e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "def add_debt_to_income_ratio(\n",
    "    df,\n",
    "    debt_col=\"Outstanding_Debt\",\n",
    "    income_col=\"Annual_Income\",\n",
    "    output_col=\"Debt_to_Income_Ratio\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Creates Debt-to-Income Ratio:\n",
    "    Outstanding_Debt / Annual_Income\n",
    "\n",
    "    - Handles NULL values\n",
    "    - Avoids division by zero\n",
    "    \"\"\"\n",
    "\n",
    "    df = df.withColumn(\n",
    "        output_col,\n",
    "        when(\n",
    "            col(debt_col).isNull() | col(income_col).isNull() | (col(income_col) == 0),\n",
    "            None\n",
    "        ).otherwise(col(debt_col) / col(income_col))\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "train_df = add_debt_to_income_ratio(train_df)\n",
    "test_df = add_debt_to_income_ratio(test_df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36f6e35c-ad03-4fba-a10f-fc8f904cd903",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Creating Total_EMI_per_month column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2e00f03-7bbd-4ff1-9dc4-7d34f61d4de2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "def add_emi_to_salary_ratio(\n",
    "    df,\n",
    "    emi_col=\"Total_EMI_per_month\",\n",
    "    salary_col=\"Monthly_Inhand_Salary\",\n",
    "    output_col=\"EMI_to_Salary_Ratio\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Creates EMI to Salary ratio feature.\n",
    "\n",
    "    Formula:\n",
    "    EMI_to_Salary_Ratio = Total_EMI_per_month / Monthly_Inhand_Salary\n",
    "\n",
    "    Handles:\n",
    "    - NULL values\n",
    "    - Division by zero\n",
    "    \"\"\"\n",
    "\n",
    "    df = df.withColumn(\n",
    "        output_col,\n",
    "        when(\n",
    "            (col(salary_col).isNull()) | (col(salary_col) == 0),\n",
    "            None\n",
    "        ).otherwise(\n",
    "            col(emi_col) / col(salary_col)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "train_df = add_emi_to_salary_ratio(train_df)\n",
    "test_df = add_emi_to_salary_ratio(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "928c4290-0f1c-424e-9555-eaa73f864fa6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Creating Saving_Capacity column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2c8fb97-5218-4da9-b236-25ad1906231e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "def add_saving_capacity(\n",
    "    df,\n",
    "    balance_col=\"Monthly_Balance\",\n",
    "    salary_col=\"Monthly_Inhand_Salary\",\n",
    "    output_col=\"Saving_Capacity\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Creates Saving Capacity feature.\n",
    "\n",
    "    Formula:\n",
    "    Saving_Capacity = Monthly_Balance / Monthly_Inhand_Salary\n",
    "\n",
    "    Handles:\n",
    "    - NULL values\n",
    "    - Division by zero\n",
    "    \"\"\"\n",
    "\n",
    "    df = df.withColumn(\n",
    "        output_col,\n",
    "        when(\n",
    "            (col(salary_col).isNull()) | (col(salary_col) == 0),\n",
    "            None\n",
    "        ).otherwise(\n",
    "            col(balance_col) / col(salary_col)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "train_df = add_saving_capacity(train_df)\n",
    "test_df = add_saving_capacity(test_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60e43961-a85c-4957-bac1-f5f1cbd98ea0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Drop Unimportant Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4ba56dd-1441-4d9d-9318-3e7015fc0703",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#We are dropping the month column because this is a classification problem, and the month feature does not provide meaningful information. However, if we were performing time-series forecasting, the month feature would be essential.\n",
    "\n",
    "cols_to_drop = [\"ID\", \"Customer_ID\", \"Name\", \"SSN\", \"month_num\", \"Payment_Behaviour\", \"Month\"]\n",
    "\n",
    "train_df = train_df.drop(*cols_to_drop)\n",
    "test_df = test_df.drop(*cols_to_drop)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a282f1b3-7910-41db-a282-d52c3ecfb216",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_df = train_df.dropna()\n",
    "test_df = test_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53d6b91c-77b8-4bb7-8981-b632acb52272",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_df.count(), test_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5f8d2cb-cc3c-48cc-8cef-48c25c303f8b",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1769792250010}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_df.limit(10).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acf5e64e-9747-4be4-85df-e4a4144d4c92",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1769883801612}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "test_df.limit(10).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3c03f2e-15d5-46b7-bb75-b58ebe837b43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Writing Silver Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14b72e3e-095d-4ab4-8c4f-fd90c2c45301",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(\n",
    "    train_df.write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"overwrite\")\n",
    "        .option(\"overwriteSchema\", \"true\")\n",
    "        .saveAsTable(\"credit_catalog.silver.train\")\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "(\n",
    "    test_df.write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"overwrite\")\n",
    "        .option(\"overwriteSchema\", \"true\")\n",
    "        .saveAsTable(\"credit_catalog.silver.test\")\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "025eb2f5-6540-4556-b0ab-c4f2d55f76d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "silver_layer",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
